{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTtKRPaWEKKm",
        "outputId": "0c5a2289-9281-46f1-e243-6cca9daf9a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9lqPizZOuQlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de bibliotecas\n"
      ],
      "metadata": {
        "id": "TTOOOZdSuTqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa seção faremos as importações das bibliotecas nescessarias para o projeto:\n",
        "\n",
        "\n",
        "\n",
        "*   *NumPy:*  é uma biblioteca para a linguagem de programação Python, que suporta o processamento de grandes, multi-dimensionais arranjos e matrizes, juntamente com uma grande coleção de funções matemáticas de alto nível para operar sobre estas matrizes.\n",
        "\n",
        "*   *TensorFlow:* é uma biblioteca de código aberto para aprendizado de máquina aplicável a uma ampla variedade de tarefas. É um sistema para criação e treinamento de redes neurais para detectar e decifrar padrões e correlações, análogo à forma como humanos aprendem e raciocinam.\n"
      ],
      "metadata": {
        "id": "sVf4veawucYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1tr2WpDDN0r-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.utils import load_img\n",
        "from keras.metrics import Accuracy, Precision, Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IyzLBZbN8mW"
      },
      "source": [
        "#### Definindo hiperparâmetro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa seção fizemos a pré definição dos parâmetros a serem utilizados durante a construção de rede:\n",
        "\n",
        "\n",
        "\n",
        "*   *Batch size:* Representa o valor da quantidade de imagens que serão analisadas por pacote.\n",
        "*   *Epochs:* Determina a quantidade de épocas na qual o modelo será treinado antes de finalizar o processo.\n",
        "*   *Input shape:* Esse input determinará o tamanho que cada imagem terá sendo **Largura x Altura x Quantidade de Cores**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eyIozINav1z2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rWe1pZGPN3ZH"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 15\n",
        "input_shape = (150, 150, 3)  # Tamanho das imagens de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkd1sNvNOBY5"
      },
      "source": [
        "###geradores de dados para pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse momento será realizada a captação, normalização e divisão entre dados de teste e validação, com a utilização da ferramenta **ImageDataGenerator**, por meio da função **flow_from_directory**.\n",
        "\n",
        "\n",
        "\n",
        "*   Path\n",
        "*   target_size\n",
        "*   Shuffle\n",
        "*   batch_size\n",
        "*   Class_mode\n",
        "*   Subset\n",
        "\n"
      ],
      "metadata": {
        "id": "snlc0MhCx_AE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFl1pOlxOBHj",
        "outputId": "3a92ae15-f7c1-43c6-b2b2-8822c4500e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3664 images belonging to 2 classes.\n",
            "Found 1568 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "data_generator = ImageDataGenerator(rescale=1./255, validation_split=(0.3))\n",
        "path = '/content/drive/MyDrive/Pasta sem nome/train'\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    path,\n",
        "    target_size=input_shape[:2],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training')\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    path,\n",
        "    target_size=input_shape[:2],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NkT84P03z817"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando parâmetros de CallBack"
      ],
      "metadata": {
        "id": "p9ccUrG30E-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A criação de **callbacks** por meio de funções presentes no modulo **tensorflow** é um metodo que auxilia na melhor execução de **RN's**\n",
        "\n",
        "\n",
        "\n",
        "*   ***ModelChechpoint:*** capturam o valor exato de todos os parâmetros (tf. Objetos variáveis) usados ​​por um modelo.\n",
        "*   ***ReduceLROnPlateau:*** redução de Learning Reating, é uma forma de conferencia de platô local durante os testes, levando a uma melhor metrica de avaliação.\n",
        "*   ***EarlyStopping:*** realiza a parada do teste caso as metricas não tenham melhora em determinada quantidade de *Epochs*.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Uwc_ki10RFv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6QdQO9d3N-8P"
      },
      "outputs": [],
      "source": [
        "filepeath = 'transferlearning_weigths.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepeath, monitor='val_loss', verbose=1, save_best_only=True, mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "alTd2dsKObTz"
      },
      "outputs": [],
      "source": [
        "lr_reduce = ReduceLROnPlateau(monitor= 'val_loss', factor=0.8, min_delta= 0.001, patience= 3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1EnDQX5IiBoT"
      },
      "outputs": [],
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience= 5, verbose=1, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fiks_cp8Ocju"
      },
      "outputs": [],
      "source": [
        "callbacks = [ lr_reduce, checkpoint, early_stop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsL2rycOVxT"
      },
      "source": [
        "###Construindo o modelo da CNC (REDES NEURAIS CONVOLUCIONAIS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dentre os varios metodos de construção de modelos de CNC, foi feito uso de um modelo ***Sequential*** com 7 camadas e redução de parte dos parâmetros usando a função ***Dropout***\n",
        "\n",
        "*   **Conv2d**: Camada de convolução 2D, esta camada cria um kernel de convolução que é enrolado com entrada de camadas que ajuda a produzir saídas.\n",
        "*   **MaxPolling2D**: Reduz a amostra da entrada ao longo de suas dimensões espaciais (altura e largura) obtendo o valor máximo em uma janela de entrada (de tamanho definido por pool_size) para cada canal da entrada.\n",
        "*   **Dropuot**: define aleatoriamente as unidades de entrada como 0 com uma frequência de taxa em cada etapa durante o tempo de treinamento, o que ajuda a evitar o overfitting. As entradas não definidas como 0 são escaladas em 1/(1 - taxa) de modo que a soma de todas as entradas permaneça inalterada."
      ],
      "metadata": {
        "id": "b92qRJkq9qcm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DloSDY_Oel8",
        "outputId": "7241a35e-3bae-4685-9204-0aafd5a1ff6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 74, 74, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 36, 36, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 165888)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               42467584  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42,543,746\n",
            "Trainable params: 42,543,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input_shape ))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVzpet_OjTS"
      },
      "source": [
        "### COMPILAÇÃO DO MODELO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A compilação do modelo é a parte do processo responsavel pela escolha de Optimizer, perca e as metricas a serem avaliadas durante os testes.\n",
        "\n",
        "*   **Optimizer**: ***adam*** é um método de descida de gradiente estocástico baseado na estimativa adaptativa de momentos de primeira e segunda ordem.\n",
        "*   **Loss**: ***binary_crossentropy*** é uma métrica de modelo que rastreia a rotulagem incorreta da classe de dados por um modelo, penalizando o modelo se ocorrerem desvios na probabilidade de classificar os rótulo.\n",
        "*   **metrics**: ***accuracy,precision,recall*** são metricas de avaliação padronizadas e nescessarias para avaliar qualquer teste."
      ],
      "metadata": {
        "id": "C-wQvdL4AgXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gNRwtyiMOglA"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CVRA4KsOqUf"
      },
      "source": [
        "### TREINO DO MODELO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O treino do modelo, representa a parte do projeto responsavel pelos testes e aprendizado do modelo com base nas metricas pré compiladas\n",
        "\n",
        "*   **steps_per_epoch**: Número total de etapas (lotes de amostras) antes de declarar uma época concluída e iniciar a próxima época\n",
        "*   **validation_data**: recebe os comparadores para validação dos testes durante as passagens de época no teste\n",
        "*   **validation_steps**: Número total de etapas (lotes de amostras) a serem coletadas antes de parar ao executar a validação no final de cada época.\n",
        "*   **callbacks**: Lista de retornos de chamada a serem aplicados durante a avaliação."
      ],
      "metadata": {
        "id": "o29SJbrnC4KW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6C2AbgyOmxO",
        "outputId": "23e91d0a-584c-4613-9976-5f4b0e4c3f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8549 - precision: 0.8525 - recall: 0.8499 \n",
            "Epoch 1: val_loss improved from -inf to 0.20844, saving model to transferlearning_weigths.hdf5\n",
            "114/114 [==============================] - 1756s 15s/step - loss: 0.4847 - accuracy: 0.8549 - precision: 0.8525 - recall: 0.8499 - val_loss: 0.2084 - val_accuracy: 0.9228 - val_precision: 0.9205 - val_recall: 0.9228 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9568 - precision: 0.9522 - recall: 0.9548\n",
            "Epoch 2: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 565s 5s/step - loss: 0.1220 - accuracy: 0.9568 - precision: 0.9522 - recall: 0.9548 - val_loss: 0.1812 - val_accuracy: 0.9388 - val_precision: 0.9399 - val_recall: 0.9375 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9598 - precision: 0.9600 - recall: 0.9587\n",
            "Epoch 3: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 562s 5s/step - loss: 0.1089 - accuracy: 0.9598 - precision: 0.9600 - recall: 0.9587 - val_loss: 0.1375 - val_accuracy: 0.9509 - val_precision: 0.9534 - val_recall: 0.9522 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9733 - precision: 0.9717 - recall: 0.9736\n",
            "Epoch 4: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 556s 5s/step - loss: 0.0803 - accuracy: 0.9733 - precision: 0.9717 - recall: 0.9736 - val_loss: 0.1461 - val_accuracy: 0.9477 - val_precision: 0.9483 - val_recall: 0.9477 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9782 - precision: 0.9779 - recall: 0.9766\n",
            "Epoch 5: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 544s 5s/step - loss: 0.0588 - accuracy: 0.9782 - precision: 0.9779 - recall: 0.9766 - val_loss: 0.1347 - val_accuracy: 0.9496 - val_precision: 0.9474 - val_recall: 0.9528 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9827 - precision: 0.9816 - recall: 0.9832\n",
            "Epoch 6: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 529s 5s/step - loss: 0.0519 - accuracy: 0.9827 - precision: 0.9816 - recall: 0.9832 - val_loss: 0.1503 - val_accuracy: 0.9483 - val_precision: 0.9471 - val_recall: 0.9483 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9879 - precision: 0.9884 - recall: 0.9857\n",
            "Epoch 7: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 535s 5s/step - loss: 0.0414 - accuracy: 0.9879 - precision: 0.9884 - recall: 0.9857 - val_loss: 0.1406 - val_accuracy: 0.9477 - val_precision: 0.9505 - val_recall: 0.9439 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9923 - precision: 0.9920 - recall: 0.9915\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "\n",
            "Epoch 8: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 559s 5s/step - loss: 0.0243 - accuracy: 0.9923 - precision: 0.9920 - recall: 0.9915 - val_loss: 0.1500 - val_accuracy: 0.9560 - val_precision: 0.9577 - val_recall: 0.9534 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9937 - precision: 0.9931 - recall: 0.9937\n",
            "Epoch 9: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 539s 5s/step - loss: 0.0185 - accuracy: 0.9937 - precision: 0.9931 - recall: 0.9937 - val_loss: 0.1535 - val_accuracy: 0.9534 - val_precision: 0.9541 - val_recall: 0.9541 - lr: 8.0000e-04\n",
            "Epoch 10/15\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970 - precision: 0.9975 - recall: 0.9970\n",
            "Epoch 10: val_loss did not improve from 0.20844\n",
            "114/114 [==============================] - 542s 5s/step - loss: 0.0100 - accuracy: 0.9970 - precision: 0.9975 - recall: 0.9970 - val_loss: 0.1552 - val_accuracy: 0.9630 - val_precision: 0.9631 - val_recall: 0.9643 - lr: 8.0000e-04\n",
            "Epoch 10: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e4c566eaa10>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.fit(train_generator,\n",
        "          steps_per_epoch=train_generator.samples//batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=validation_generator.samples//batch_size,\n",
        "          callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UltUjBRcOxr0"
      },
      "source": [
        "###AVALIÇÃO DO MODELO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A avaliação do modelo representa a etapa responsavel por quantificar as metricas avaliativas do modelo testado.\n",
        "\n",
        "*   **Accuracy**\n",
        "*   **Precision**\n",
        "*   **recall**\n",
        "*   **F1_score**"
      ],
      "metadata": {
        "id": "xjMKMAQCGJqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y6GNLajONn1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ed295e-af37-46ea-c4b1-d1b5de3ff201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49/49 [==============================] - 63s 1s/step - loss: 0.1552 - accuracy: 0.9630 - precision: 0.9631 - recall: 0.9643\n"
          ]
        }
      ],
      "source": [
        "# Avalie o modelo nos dados de teste\n",
        "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(validation_generator, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_lCnkxV2PJUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2568748e-5fd6-4d4e-ff27-a7b92342af77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perda de teste: 0.15521292388439178\n",
            "Acurácia de teste: 0.9630101919174194\n",
            "Precisão de teste: 0.9630573391914368\n",
            "Teste de Recall 0.9642857313156128\n"
          ]
        }
      ],
      "source": [
        "print('Perda de teste:', test_loss)\n",
        "print('Acurácia de teste:', test_accuracy)\n",
        "print('Precisão de teste:', test_precision)\n",
        "print('Teste de Recall', test_recall)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score_divisor= test_precision * test_recall\n",
        "f1_score_dividendo = test_precision + test_recall"
      ],
      "metadata": {
        "id": "n-OLyHzbGBzr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = (2*f1_score_divisor) / f1_score_dividendo\n",
        "f1"
      ],
      "metadata": {
        "id": "ZEXn8735IW97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e2e567-65c1-425e-b4a0-702098776d1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9636711437956588"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kA5ootEcJSUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}